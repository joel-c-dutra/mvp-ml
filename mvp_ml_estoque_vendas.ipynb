{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PUC Rio | MVP | Machine Learning\n",
        "\n",
        "Aluno: Joel Carneiro Dutra\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AyoOdAxdp6_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Descrição do Problema:**\n",
        "\n",
        "Uma empresa que possui várias franquias de lojas, precisa prever a quantidade de produtos que cada franquia precisará para manter seus estoques otimizados. O objetivo é garantir que cada franquia tenha o estoque adequado para atender à demanda de seus clientes, minimizando ao mesmo tempo os custos de armazenamento e o risco de falta de produtos."
      ],
      "metadata": {
        "id": "uWF_3T3Hp2KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 1: Coleta de Dados**\n",
        "\n",
        "A empresa coleta dados históricos de vendas e estoque de cada uma das franquias, juntamente com informações sobre sazonalidade, promoções, dados geográficos, entre outros fatores que podem influenciar a demanda.\n",
        "\n",
        "Esta etapa foi realizada por meio de uma extração no Google BigQuery, onde foram selecionadas as dimensões necessárias para o treinamento do modelo. A base possui um histórico de 10 ítens vendidos no ano de 2023, na região Sudeste, o estoque das franquias que efetuaram as vendas. A partir deste ponto, foi realizado o upload da base."
      ],
      "metadata": {
        "id": "cdSc84z0seSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração para não exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Imports necessários\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as ms\n",
        "from matplotlib import cm\n",
        "from pandas import set_option\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest # para a Seleção Univariada\n",
        "from sklearn.feature_selection import f_classif # para o teste ANOVA da Seleção Univariada\n",
        "from sklearn.feature_selection import RFE # para a eliminação recursiva de atributos\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression # Regressão Logística, para a eliminação recursiva de atributos\n",
        "\n",
        "from sklearn.model_selection import train_test_split # para particionar em bases de treino e teste (holdout)\n",
        "from sklearn.model_selection import KFold # para preparar os folds da validação cruzada\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score # para executar a validação cruzada\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score # para a exibição da acurácia do modelo\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier # algoritmo Árvore de Classificação\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC # algoritmo SVM\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier # Random Forest\n",
        "from sklearn.ensemble import ExtraTreesClassifier # ExtraTrees\n",
        "from sklearn.ensemble import GradientBoostingClassifier # Gradient Boosting"
      ],
      "metadata": {
        "id": "gkpWUjzvuSqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GfKPjC61yace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44839cc7-1568-4cc8-eb6a-a55638c055c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n",
        "!git config --global user.name \"Joel-Dutra\"\n",
        "!git config --global user.email \"dutra.jc@hotmail.com\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0CIGELIxnOM",
        "outputId": "6ddbf468-c698-4e6b-d667-0f464fe102e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"Primeiro commit\"\n",
        "!git remote add origin https://github.com/Joel-Dutra/Joel-Dutra.git\n",
        "!git push -u origin master\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK_TYc8nyUl3",
        "outputId": "a811b959-417b-4e5e-9458-ad16b784a6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: open(\"drive/MyDrive/Anhembi Morumbi/N1 - Comércio Eletrônico.gdoc\"): Operation not supported\n",
            "error: unable to index file 'drive/MyDrive/Anhembi Morumbi/N1 - Comércio Eletrônico.gdoc'\n",
            "fatal: adding files failed\n",
            "On branch master\n",
            "\n",
            "Initial commit\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mdrive/\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "error: src refspec master does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/Joel-Dutra/Joel-Dutra.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "caminho_arquivo = '/content/drive/My Drive/PUC-Rio/base_historica.csv'"
      ],
      "metadata": {
        "id": "OJ0KHfUTylJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Leitura do arquivo CSV em um DataFrame\n",
        "dataset = pd.read_csv(caminho_arquivo)"
      ],
      "metadata": {
        "id": "sr9AsQQbzPE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "Cng9JpAG01tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica o tipo de dataset de cada atributo\n",
        "dataset.dtypes"
      ],
      "metadata": {
        "id": "k9kdvPg9wWzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz um resumo estatístico do dataset (média, desvio padrão, mínimo, máximo e os quartis)\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "AeO2xiLbwc1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verificando nulls no dataset\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "PfEOKNcb2DNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribuição da(s) classe(s)\n",
        "print(dataset.groupby('estoque_loja').size())"
      ],
      "metadata": {
        "id": "tjrEgl5K61OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# salvando um novo dataset para tratamento de missings\n",
        "\n",
        "# recuperando os nomes das colunas\n",
        "col = list(dataset.columns)\n",
        "\n",
        "# o novo dataset irá conter todas as colunas com exceção da última (classe)\n",
        "atributos = dataset[col[0:-1]]\n",
        "\n",
        "# substituindo os zeros por NaN\n",
        "atributos.replace(0, np.nan, inplace=True)\n",
        "\n",
        "# exibindo visualização matricial da nulidade do dataset\n",
        "ms.matrix(atributos)"
      ],
      "metadata": {
        "id": "1yUxfRr12XMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removendo as colunas 'qtde_venda', 'qt_dias_sem_estoque' e 'qt_dias_sem_estoque_aberta'\n",
        "atributos.drop(['qtde_venda', 'qt_dias_sem_estoque', 'qt_dias_sem_estoque_aberta'], axis=1, inplace= True)\n",
        "\n",
        "# exibindo visualização matricial da nulidade do dataset\n",
        "ms.matrix(atributos)"
      ],
      "metadata": {
        "id": "7qCOBL1P3ifk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# substituindo os NaN de 'preg' por 0\n",
        "atributos['qt_dias_loja_fechada'].fillna(0, inplace=True)\n",
        "\n",
        "# substituindo os NaN de 'plas', 'pres'e 'mass' pela mediana da coluna\n",
        "atributos['qt_dias_com_estoque'].fillna(atributos['qt_dias_com_estoque'].median(), inplace=True)\n",
        "atributos['qt_dias_com_estoque_aberta'].fillna(atributos['qt_dias_com_estoque_aberta'].median(), inplace=True)\n",
        "\n",
        "# exibindo visualização matricial da nulidade do dataset\n",
        "ms.matrix(atributos)"
      ],
      "metadata": {
        "id": "x2BM1wuT40Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardando o novo dataset para testes futuros\n",
        "datasetSemMissings = atributos\n",
        "\n",
        "# exibindo as primeiras linhas\n",
        "datasetSemMissings.head()"
      ],
      "metadata": {
        "id": "Z9N2qRy56AlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 0.20\n",
        "seed = 3\n",
        "\n",
        "# Separação em conjuntos de treino e teste (dataset original)\n",
        "array = dataset.values\n",
        "X = array[:,0:4]\n",
        "y = array[:,4]\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "#    test_size=test_size, shuffle=True, random_state=seed) # sem estratificação\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "    test_size=test_size, shuffle=True, random_state=seed, stratify=y) # com estratificação"
      ],
      "metadata": {
        "id": "4DYsmR_67oUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separação em conjuntos de treino e teste (dataset sem missings - 2 colunas a menos!)\n",
        "array = datasetSemMissings.values\n",
        "X_sm = array[:,0:2]\n",
        "y_sm = array[:,2]\n",
        "#X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm, y_sm,\n",
        "#    test_size=test_size, shuffle=True, random_state=seed) # sem estratificação\n",
        "X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm, y_sm,\n",
        "    test_size=test_size, shuffle=True, random_state=seed, stratify=y_sm) # com estratificação"
      ],
      "metadata": {
        "id": "4HRUoC7Z767w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_atributos = dataset[['qtde_venda','qt_dias_com_estoque','qt_dias_sem_estoque','qt_dias_com_estoque_aberta','qt_dias_sem_estoque_aberta','qt_dias_loja_fechada','estoque_loja']]"
      ],
      "metadata": {
        "id": "Q2JOAXwV8ySg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_atributos.head()"
      ],
      "metadata": {
        "id": "jqP_lgCPFxBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histograma\n",
        "dataset_atributos.hist(figsize = (15,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZEQB_hHRw-4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Density Plot\n",
        "dataset_atributos.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False, figsize = (15,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sjyrZ_eAxVYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de Correlação com Matplotlib Seaborn\n",
        "sns.heatmap(dataset_atributos.corr(), annot=True, cmap='RdBu');"
      ],
      "metadata": {
        "id": "0nM5B5hXyLT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter Plot com Seaborn - Variação 1\n",
        "\n",
        "sns.pairplot(dataset_atributos)"
      ],
      "metadata": {
        "id": "EdYJ_LpqzAeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verificando nulls no dataset\n",
        "dataset_atributos.isnull().sum()"
      ],
      "metadata": {
        "id": "adMU8Yf_1dQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparação dos dados\n",
        "\n",
        "# Separação em bases de treino e teste (holdout)\n",
        "array = dataset_atributos.values\n",
        "X = array[:,0:6] # atributos\n",
        "y = array[:,6]   # classe (target)"
      ],
      "metadata": {
        "id": "kdcPXVL2F7h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SelectKBest\n",
        "\n",
        "# Seleção de atributos com SelectKBest\n",
        "best_var = SelectKBest(score_func=f_classif, k=4)\n",
        "\n",
        "# Executa a função de pontuação em (X, y) e obtém os atributos selecionados\n",
        "fit = best_var.fit(X, y)\n",
        "\n",
        "# Reduz X para os atributos selecionados\n",
        "features = fit.transform(X)\n",
        "\n",
        "# Resultados\n",
        "print('\\nNúmero original de atributos:', X.shape[1])\n",
        "print('\\nNúmero reduzido de atributos:', features.shape[1])\n",
        "\n",
        "# Exibe os atributos orginais\n",
        "print(\"\\nAtributos Originais:\", dataset_atributos.columns[0:7])\n",
        "\n",
        "# Exibe as pontuações de cada atributos e os 4 escolhidos (com as pontuações mais altas)\n",
        "np.set_printoptions(precision=3) # 3 casas decimais\n",
        "print(\"\\nScores dos Atributos Originais:\", fit.scores_)\n",
        "print(\"\\nAtributos Selecionados:\", best_var.get_feature_names_out(input_features=dataset_atributos.columns[0:6]))"
      ],
      "metadata": {
        "id": "rhGzCt_LGyr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminação Recursiva de Atributos\n",
        "\n",
        "# Criação do modelo\n",
        "modelo = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Eliminação Recursiva de Atributos\n",
        "rfe = RFE(modelo, n_features_to_select=3)\n",
        "fit = rfe.fit(X, y)\n",
        "\n",
        "# Print dos resultados\n",
        "print(\"Atributos Originais:\", dataset_atributos.columns[0:7])\n",
        "\n",
        "# Exibe os atributos selecionados (marcados como True em \"Atributos Selecionados\"\n",
        "# e com valor 1 em \"Ranking dos Atributos\")\n",
        "print(\"\\nAtributos Selecionados: %s\" % fit.support_)\n",
        "print(\"\\nRanking de atributos: %s\" % fit.ranking_)\n",
        "print(\"\\nQtd de melhores Atributos: %d\" % fit.n_features_)\n",
        "print(\"\\nNomes dos Atributos Selecionados: %s\" % fit.get_feature_names_out(input_features=dataset_atributos.columns[0:6]))"
      ],
      "metadata": {
        "id": "PNPhosbUz0Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importância de Atributos com ExtraTrees\n",
        "\n",
        "# Criação do modelo para seleção de atributos\n",
        "modelo = ExtraTreesClassifier(n_estimators=100)\n",
        "modelo.fit(X,y)\n",
        "\n",
        "# Exibe os atributos orginais\n",
        "print(\"\\nAtributos Originais:\", dataset_atributos.columns[0:7])\n",
        "\n",
        "# Exibe a pontuação de importância para cada atributo (quanto maior a pontuação, mais importante é o atributo).\n",
        "print(modelo.feature_importances_)"
      ],
      "metadata": {
        "id": "6c6ZX6yuSdH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "SelectKBest:\n",
        "['qt_dias_com_estoque' 'qt_dias_sem_estoque' 'qt_dias_com_estoque_aberta' 'qt_dias_sem_estoque_aberta']\n",
        "\n",
        "Logistic Regression:\n",
        "['qtde_venda' 'qt_dias_sem_estoque' 'qt_dias_sem_estoque_aberta']\n",
        "\n",
        "ExtraTress:\n",
        "['qtde_venda' 'qt_dias_com_estoque' 'qt_dias_sem_estoque']"
      ],
      "metadata": {
        "id": "D2rqmoB0sJsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.dtypes"
      ],
      "metadata": {
        "id": "Y41iSzygsUMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe()"
      ],
      "metadata": {
        "id": "r9KGCIn5v7oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZXPmP2YarzF0"
      }
    }
  ]
}