{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cdSc84z0seSG",
        "Jqe_14a6RKJL",
        "9B-JnkaURYnK",
        "RTF-H1btRuQG",
        "ynqVR0grClS5",
        "09H7GRVdWiZW",
        "vVA9EvpaXFoe",
        "5FTHR1rw8Heo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PUC Rio | MVP | Machine Learning\n",
        "\n",
        "Aluno: Joel Carneiro Dutra\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AyoOdAxdp6_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Definição do Problema:**\n",
        "\n",
        "Uma empresa que possui várias franquias de lojas, precisa prever a quantidade de produtos que cada franquia precisará para manter seus estoques otimizados. O objetivo é garantir que cada franquia tenha o estoque adequado para atender à demanda de seus clientes, minimizando ao mesmo tempo os custos de armazenamento e o risco de falta de produtos."
      ],
      "metadata": {
        "id": "uWF_3T3Hp2KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 1: Coleta de Dados**\n",
        "\n",
        "A empresa coleta dados históricos de vendas e estoque de cada uma das franquias, juntamente com informações sobre sazonalidade, promoções, dados geográficos, entre outros fatores que podem influenciar a demanda.\n",
        "\n",
        "Esta etapa foi realizada por meio de uma extração no Google BigQuery, contendo os seguintes campos:\n",
        "\n",
        "- **dt_venda** - Data em que a venda foi realizada\n",
        "- **loja** - Código das franquias que realizaram as vendas\n",
        "- **uf** - UF das franquias contendo Rio de Janeiro e São Paulo\n",
        "- **produto** - Descrição do produto vendido (Produto X, Y e Z)\n",
        "- **canal_venda** - Canal onde ocorreu a venda (Loja ou Site)\n",
        "- **tipo_venda** - Tipo de venda sendo Promoção ou Regular\n",
        "- **vlr_venda** - Valor total da venda\n",
        "- **qt_venda** - Quantidade de itens vendidos\n",
        "\n",
        "Atributos:\n",
        "- **qt_dias_com_estoque** - Quantidade de dias em que a loja tinha estoque\n",
        "- **qt_dias_sem_estoque** - Quantidade de dias em que a loja não tinha estoque\n",
        "- **qt_dias_com_estoque_aberta** - Quantidade de dias em que a loja tinha estoque e estava aberta para venda\n",
        "- **qt_dias_sem_estoque_aberta** - Quantidade de dias em que a loja não tinha estoque e estava aberta para venda\n",
        "- **qt_dias_loja_fechada** - Quantidade de dias em que a loja esteve fechada\n",
        "- **estoque_loja** - Quantidade de estoque da loja\n",
        "- **habilitador** - Determina se a loja deverá ser abastecida caso tenha menos que 5 itens no estoque (variável 1 ou 0)"
      ],
      "metadata": {
        "id": "cdSc84z0seSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração para não exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Imports necessários\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as ms\n",
        "from matplotlib import cm\n",
        "from pandas import set_option\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "gkpWUjzvuSqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 2: Carga dos Dados**\n",
        "\n",
        "Nessa etapa fazemos a conexão do Colab ao dataset (Google Drive), fazendo a requisição do arquivo em formato .csv e a leitura do arquivo em um DataFrame."
      ],
      "metadata": {
        "id": "Jqe_14a6RKJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "caminho_arquivo = '/content/drive/My Drive/PUC-Rio/base_historica.csv'\n",
        "\n",
        "# Leitura do arquivo CSV em um DataFrame\n",
        "dataset = pd.read_csv(caminho_arquivo)\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "GfKPjC61yace"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 3: Análise dos Dados**\n",
        "\n",
        "Nesta etapa é apresentado algumas informações do dataset, tais como:\n",
        "\n",
        "- Volume de registros (shape);\n",
        "- Tipo de dado de cada atributo (dtypes);\n",
        "- Descrição dos campos (describe)."
      ],
      "metadata": {
        "id": "9B-JnkaURYnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostra as dimensões do dataset\n",
        "print(dataset.shape)"
      ],
      "metadata": {
        "id": "BGgg56Wa-GHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostra as informações do dataset\n",
        "print(dataset.info())"
      ],
      "metadata": {
        "id": "v_HtGK4A-LiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "Cng9JpAG01tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica o tipo de dataset de cada atributo\n",
        "dataset.dtypes"
      ],
      "metadata": {
        "id": "k9kdvPg9wWzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz um resumo estatístico do dataset (média, desvio padrão, mínimo, máximo e os quartis)\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "AeO2xiLbwc1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 4: Pré-Processamento dos Dados**\n",
        "\n",
        "Nesta etapa realizamos os seguintes processos:\n",
        "\n",
        "- Verificação por campos nulos (isnull);\n",
        "- Distribuição da classe (groupby);\n",
        "- Tratamento de Missings e Limpeza (replace);\n",
        "- Histograma e Matriz de Correlação (plot);\n",
        "- Feature Selection (SelectKBest, Recursiva e ExtraTrees)."
      ],
      "metadata": {
        "id": "RTF-H1btRuQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verificando nulls no dataset\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "PfEOKNcb2DNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribuição da classe\n",
        "print(dataset.groupby('habilitador').size())"
      ],
      "metadata": {
        "id": "tjrEgl5K61OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# salvando um novo dataset para tratamento de missings\n",
        "\n",
        "# recuperando os nomes das colunas\n",
        "col = list(dataset.columns)\n",
        "\n",
        "# o novo dataset irá conter todas as colunas com exceção da última (classe)\n",
        "atributos = dataset[col[0:-1]]\n",
        "\n",
        "# substituindo os zeros por NaN\n",
        "atributos.replace(0, np.nan, inplace=True)\n",
        "\n",
        "# exibindo visualização matricial da nulidade do dataset\n",
        "ms.matrix(atributos)"
      ],
      "metadata": {
        "id": "1yUxfRr12XMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removendo as colunas 'vlr_venda' e 'qt_venda'\n",
        "atributos.drop(['vlr_venda', 'qt_venda'], axis=1, inplace= True)\n",
        "\n",
        "# exibindo visualização matricial da nulidade do dataset\n",
        "ms.matrix(atributos)"
      ],
      "metadata": {
        "id": "b0jc955IfC0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# substituindo os NaN por 0\n",
        "atributos['qt_dias_sem_estoque'].fillna(0, inplace=True)\n",
        "atributos['qt_dias_sem_estoque_aberta'].fillna(0, inplace=True)\n",
        "atributos['estoque_loja'].fillna(0, inplace=True)\n",
        "\n",
        "# substituindo os NaN de 'qt_dias_com_estoque' e 'qt_dias_com_estoque_aberta' pela mediana da coluna\n",
        "atributos['qt_dias_com_estoque'].fillna(atributos['qt_dias_com_estoque'].median(), inplace=True)\n",
        "atributos['qt_dias_com_estoque_aberta'].fillna(atributos['qt_dias_com_estoque_aberta'].median(), inplace=True)\n",
        "atributos['qt_dias_loja_fechada'].fillna(atributos['qt_dias_loja_fechada'].median(), inplace=True)\n",
        "\n",
        "# exibindo visualização matricial da nulidade do dataset\n",
        "ms.matrix(atributos)"
      ],
      "metadata": {
        "id": "x2BM1wuT40Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardando o novo dataset para testes futuros\n",
        "datasetSemMissings = atributos\n",
        "\n",
        "# incluindo a coluna 'class' no novo dataset\n",
        "datasetSemMissings['habilitador'] = dataset['habilitador']\n",
        "\n",
        "# exibindo as primeiras linhas\n",
        "datasetSemMissings.head()"
      ],
      "metadata": {
        "id": "X-GrFwAw07iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Referenciando os atribuitos a uma nova variável\n",
        "dataset_atributos = datasetSemMissings.loc[:, ['qt_dias_com_estoque', 'qt_dias_sem_estoque', 'qt_dias_com_estoque_aberta', 'qt_dias_sem_estoque_aberta', 'qt_dias_loja_fechada', 'estoque_loja', 'habilitador']]\n",
        "dataset_atributos.head()"
      ],
      "metadata": {
        "id": "hlgNurWR4bP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histograma\n",
        "dataset_atributos.hist(figsize = (15,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZEQB_hHRw-4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Density Plot\n",
        "dataset_atributos.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False, figsize = (15,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sjyrZ_eAxVYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de Correlação com Matplotlib Seaborn\n",
        "sns.heatmap(dataset_atributos.corr(), annot=True, cmap='RdBu');"
      ],
      "metadata": {
        "id": "0nM5B5hXyLT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Feature Selection **"
      ],
      "metadata": {
        "id": "b4aWbMz8SOA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparação dos dados para Feature Selection\n",
        "\n",
        "# Separação em bases de treino e teste (holdout)\n",
        "array = dataset_atributos.values\n",
        "X = array[:,0:6] # atributos\n",
        "y = array[:,6]   # classe (target)"
      ],
      "metadata": {
        "id": "kdcPXVL2F7h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SelectKBest\n",
        "\n",
        "# Seleção de atributos com SelectKBest\n",
        "best_var = SelectKBest(score_func=f_classif, k=3)\n",
        "\n",
        "# Executa a função de pontuação em (X, y) e obtém os atributos selecionados\n",
        "fit = best_var.fit(X, y)\n",
        "\n",
        "# Reduz X para os atributos selecionados\n",
        "features = fit.transform(X)\n",
        "\n",
        "# Resultados\n",
        "print('\\nNúmero original de atributos:', X.shape[1])\n",
        "print('\\nNúmero reduzido de atributos:', features.shape[1])\n",
        "\n",
        "# Exibe os atributos orginais\n",
        "print(\"\\nAtributos Originais:\", dataset_atributos.columns[0:7])\n",
        "\n",
        "# Exibe as pontuações de cada atributos e os 4 escolhidos (com as pontuações mais altas)\n",
        "np.set_printoptions(precision=3) # 3 casas decimais\n",
        "print(\"\\nScores dos Atributos Originais:\", fit.scores_)\n",
        "print(\"\\nAtributos Selecionados:\", best_var.get_feature_names_out(input_features=dataset_atributos.columns[0:6]))"
      ],
      "metadata": {
        "id": "rhGzCt_LGyr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminação Recursiva de Atributos\n",
        "\n",
        "# Criação do modelo\n",
        "modelo = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Eliminação Recursiva de Atributos\n",
        "rfe = RFE(modelo, n_features_to_select=3)\n",
        "fit = rfe.fit(X, y)\n",
        "\n",
        "# Print dos resultados\n",
        "print(\"Atributos Originais:\", dataset_atributos.columns[0:7])\n",
        "\n",
        "# Exibe os atributos selecionados (marcados como True em \"Atributos Selecionados\"\n",
        "# e com valor 1 em \"Ranking dos Atributos\")\n",
        "print(\"\\nAtributos Selecionados: %s\" % fit.support_)\n",
        "print(\"\\nRanking de atributos: %s\" % fit.ranking_)\n",
        "print(\"\\nQtd de melhores Atributos: %d\" % fit.n_features_)\n",
        "print(\"\\nNomes dos Atributos Selecionados: %s\" % fit.get_feature_names_out(input_features=dataset_atributos.columns[0:6]))"
      ],
      "metadata": {
        "id": "PNPhosbUz0Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importância de Atributos com ExtraTrees\n",
        "\n",
        "# Criação do modelo para seleção de atributos\n",
        "modelo = ExtraTreesClassifier(n_estimators=100)\n",
        "modelo.fit(X,y)\n",
        "\n",
        "# Exibe os atributos orginais\n",
        "print(\"\\nAtributos Originais:\", dataset_atributos.columns[0:7])\n",
        "\n",
        "# Exibe a pontuação de importância para cada atributo (quanto maior a pontuação, mais importante é o atributo).\n",
        "print(modelo.feature_importances_)"
      ],
      "metadata": {
        "id": "6c6ZX6yuSdH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atributo com maior relevância durante as análises: **estoque_loja**\n",
        "\n",
        "- SelectKBest: **698.377**\n",
        "- ExtraTrees: **0.745**\n",
        "- Recursiva: **True**"
      ],
      "metadata": {
        "id": "D2rqmoB0sJsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 5: Separação em conjunto de Treino e Teste (Holdout)**\n",
        "\n",
        "Nesta etapa foram realizados os seguintes processos:\n",
        "\n",
        "- Validação cruzada\n",
        "- Teste e comparação dos modelos"
      ],
      "metadata": {
        "id": "ynqVR0grClS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Holdout com estratificação*"
      ],
      "metadata": {
        "id": "nIk08HuyUkR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 0.20 # tamanho do conjunto de teste\n",
        "seed = 7 # semente aleatória\n",
        "\n",
        "# Separação em conjuntos de treino e teste\n",
        "array = dataset_atributos.values\n",
        "X = array[:,0:6]\n",
        "y = array[:,6]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "    test_size=test_size, shuffle=True, random_state=seed) # holdout sem estratificação\n",
        "\n",
        "# Parâmetros e partições da validação cruzada\n",
        "scoring = 'accuracy'\n",
        "num_particoes = 10\n",
        "kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed) # Validação cruzada"
      ],
      "metadata": {
        "id": "L3yv7JzgFvd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Holdout sem estratificação*"
      ],
      "metadata": {
        "id": "daCoUnOsVWH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 0.20 # tamanho do conjunto de teste\n",
        "seed = 7 # semente aleatória\n",
        "\n",
        "# Separação em conjuntos de treino e teste\n",
        "array = dataset_atributos.values\n",
        "X = array[:,0:6]\n",
        "y = array[:,6]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "    test_size=test_size, shuffle=True, random_state=seed, stratify=y) # holdout com estratificação\n",
        "\n",
        "# Parâmetros e partições da validação cruzada\n",
        "scoring = 'accuracy'\n",
        "num_particoes = 10\n",
        "kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed) # Validação cruzada"
      ],
      "metadata": {
        "id": "GwkO5XniH5l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "# Lista que armazenará os módulos\n",
        "models = []\n",
        "\n",
        "# Criando os modelos e adicionando-os na lista de modelos\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('SVM', SVC()))\n",
        "\n",
        "# Definindo os parâmetros do classificador base o BaggingClassifier\n",
        "base = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "max_features = 3\n",
        "\n",
        "# Criando os modelos para o VotingClassifier\n",
        "bases = []\n",
        "model1 = DecisionTreeClassifier()\n",
        "bases.append(('cart', model1))\n",
        "model2 = SVC()\n",
        "bases.append(('svm', model2))"
      ],
      "metadata": {
        "id": "N5PR_kAaI9hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando os ensembles e adicionando-os na lista de modelos\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))"
      ],
      "metadata": {
        "id": "UBdkirwdL23p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para armazenar os resultados\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "# Avaliação dos modelos\n",
        "for name, model in models:\n",
        "  cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)"
      ],
      "metadata": {
        "id": "i5qXFpxUMluX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot de comparação dos modelos\n",
        "fig = plt.figure(figsize=(15,10))\n",
        "fig.suptitle('Comparação dos Modelos')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KexAUVT2OYd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 6: Pipeline**\n",
        "\n",
        "Nesta etapa foi feita a comparação entre os modelos:\n",
        "\n",
        "- Original\n",
        "- Padronizado\n",
        "- Normalizado"
      ],
      "metadata": {
        "id": "09H7GRVdWiZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7) # definindo uma semente global para este bloco\n",
        "\n",
        "# Lista para armazenar os pipelines e os resultados para todas as visões do dataset\n",
        "pipelines = []\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "# Criando os elementos do pipeline\n",
        "\n",
        "#Algoritmos que serão utilizados\n",
        "cart = ('CART', DecisionTreeClassifier())\n",
        "svm = ('SVM', SVC())\n",
        "random_forest = ('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "extra_trees = ('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "gradient_boosting = ('GB', GradientBoostingClassifier(n_estimators=num_trees))\n",
        "\n",
        "# Transformações que serão utilizadas\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())"
      ],
      "metadata": {
        "id": "kOuQUC62sltK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Montando os pipelines\n",
        "\n",
        "# Dataset original\n",
        "pipelines.append(('CART-orig', Pipeline([cart])))\n",
        "pipelines.append(('SVM-orig', Pipeline([svm])))\n",
        "pipelines.append(('RF-orig', Pipeline([random_forest])))\n",
        "pipelines.append(('ET-orig', Pipeline([extra_trees])))\n",
        "pipelines.append(('GB-orig', Pipeline([gradient_boosting])))\n",
        "\n",
        "# Dataset padronizado\n",
        "pipelines.append(('CART-padr', Pipeline([standard_scaler, cart])))\n",
        "pipelines.append(('SVM-padr', Pipeline([standard_scaler, svm])))\n",
        "pipelines.append(('RF-padr', Pipeline([standard_scaler, random_forest])))\n",
        "pipelines.append(('ET-padr', Pipeline([standard_scaler, extra_trees])))\n",
        "pipelines.append(('GB-padr', Pipeline([standard_scaler, gradient_boosting])))\n",
        "\n",
        "# Dataset normalizado\n",
        "pipelines.append(('CART-norm', Pipeline([min_max_scaler, cart])))\n",
        "pipelines.append(('SVM-norm', Pipeline([min_max_scaler, svm])))\n",
        "pipelines.append(('RF-norm', Pipeline([min_max_scaler, random_forest])))\n",
        "pipelines.append(('ET-norm', Pipeline([min_max_scaler, extra_trees])))\n",
        "pipelines.append(('GB-norm', Pipeline([min_max_scaler, gradient_boosting])))\n",
        "\n",
        "# Executando os pipelines\n",
        "for name, model in pipelines:\n",
        "  cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)"
      ],
      "metadata": {
        "id": "faYUA6ctuW66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot de comparação dos modelos\n",
        "fig = plt.figure(figsize=(20,6))\n",
        "fig.suptitle('Comparação dos Modelos - Dataset original, padronizado e normalizado')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names, rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4SsTE_fNyrjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 7: Avaliação do modelo**\n",
        "\n",
        "Nesta etapa foi realizado a avaliação do modelo em conjunto de teste"
      ],
      "metadata": {
        "id": "vVA9EvpaXFoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação do modelo com o conjunto de testes\n",
        "\n",
        "# Preparação do modelo\n",
        "scaler = StandardScaler().fit(X_train) # ajuste do scaler com o conjunto de treino\n",
        "rescaledX = scaler.transform(X_train) # aplicação da padronização no conjunto de treino\n",
        "model = SVC(max_iter=200)\n",
        "model.fit(rescaledX, y_train)\n",
        "\n",
        "# Estimativa da acurácia no conjunto de teste\n",
        "rescaledTestX = scaler.transform(X_test) # aplicação da padronização no conjunto de teste\n",
        "predictions = model.predict(rescaledTestX)\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "metadata": {
        "id": "4dWbHG425t2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 8: Aplicação em produção**\n",
        "\n",
        "Nesta etapa, após a escolha do modelo, foi realizado o uso do mesmo com novos dados"
      ],
      "metadata": {
        "id": "5FTHR1rw8Heo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparação do modelo com todo o dataset\n",
        "scaler = StandardScaler().fit(X) # ajuste do scaler com todo o dataset\n",
        "rescaledX = scaler.transform(X) # aplicação da padronização com todo o dataset\n",
        "model.fit(rescaledX, y)"
      ],
      "metadata": {
        "id": "Ah5VkdHZ8OIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulação com novos dados\n",
        "\n",
        "data = {'qt_dias_com_estoque': [5, 30, 10],\n",
        "        'qt_dias_sem_estoque': [3, 5, 15],\n",
        "        'qt_dias_com_estoque_aberta': [2, 20, 10],\n",
        "        'qt_dias_sem_estoque_aberta': [0, 5, 20],\n",
        "        'qt_dias_loja_fechada': [1, 7, 15],\n",
        "        'estoque_loja': [0, 2, 5],\n",
        "        }\n",
        "\n",
        "atributos = ['qt_dias_com_estoque', 'qt_dias_sem_estoque', 'qt_dias_com_estoque_aberta', 'qt_dias_sem_estoque_aberta', 'qt_dias_loja_fechada', 'estoque_loja']\n",
        "entrada = pd.DataFrame(data, columns=atributos)\n",
        "\n",
        "array_entrada = entrada.values\n",
        "X_entrada = array_entrada[:,0:6].astype(float)\n",
        "\n",
        "# Padronização nos dados de entrada usando o scaler utilizado em X\n",
        "rescaledEntradaX = scaler.transform(X_entrada)\n",
        "print(rescaledEntradaX)"
      ],
      "metadata": {
        "id": "bixW34219QM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predição de classes dos dados de entrada\n",
        "\n",
        "saidas = model.predict(rescaledEntradaX)\n",
        "print(saidas)"
      ],
      "metadata": {
        "id": "uj_EbrO6AaaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusão**\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "zd8qHWzaXtlS"
      }
    }
  ]
}